{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461fd0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU 0: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0073802f-a3b0-4578-8549-96784ca4b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\", transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.no_annotation_count = 0\n",
    "\n",
    "        images_dir = os.path.join(root_dir, \"raw-images\")\n",
    "        annotations_dir = os.path.join(root_dir, \"annotations\")\n",
    "\n",
    "        for subfolder in os.listdir(images_dir):\n",
    "            subfolder_images_dir = os.path.join(images_dir, subfolder)\n",
    "            subfolder_annotations_dir = os.path.join(annotations_dir, subfolder)\n",
    "\n",
    "            if not os.path.isdir(subfolder_annotations_dir):\n",
    "                continue\n",
    "\n",
    "            image_files = [f for f in os.listdir(subfolder_images_dir) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "            \n",
    "            # Include only files with \"rgb\" in their name\n",
    "            image_files = [f for f in image_files if \"rgb\" in f.lower()]\n",
    "\n",
    "            image_files.sort()\n",
    "\n",
    "            num_images = len(image_files)\n",
    "            if split == \"train\":\n",
    "                image_files = image_files[:int(0.7 * num_images)]\n",
    "            elif split == \"val\":\n",
    "                image_files = image_files[int(0.7 * num_images):int(0.9 * num_images)]\n",
    "            elif split == \"test\":\n",
    "                image_files = image_files[int(0.9 * num_images):]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid split: {split}\")\n",
    "\n",
    "            for filename in image_files:\n",
    "                image_path = os.path.join(subfolder_images_dir, filename)\n",
    "                annotation_path = os.path.join(subfolder_annotations_dir, os.path.splitext(filename)[0] + \".xml\")\n",
    "\n",
    "                if os.path.exists(annotation_path):\n",
    "                    tree = ET.parse(annotation_path)\n",
    "                    root = tree.getroot()\n",
    "                    label = root.find(\"object\").find(\"name\").text\n",
    "\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(label)\n",
    "                else:\n",
    "                    self.no_annotation_count += 1\n",
    "\n",
    "        print(f\"Number of images: {len(self.images)}\")\n",
    "        print(f\"Number of annotations: {len(self.labels)}\")\n",
    "        print(f\"Number of images without annotations: {self.no_annotation_count}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.labels[index]\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a090a-905e-4280-a59e-1c3736adf210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688f3239-66c6-4a54-ab3b-b5b2d7f4912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 227\n",
      "Number of annotations: 227\n",
      "Number of images without annotations: 98\n",
      "Class labels: {'largeCar': 0, 'lightTruck': 1, 'smallCar': 2, 'heavyTruck': 3}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Preparation\n",
    "dataset_path = \"dataset\"\n",
    "split = \"train\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(dataset_path, split, transform)\n",
    "\n",
    "class_labels = list(set(dataset.labels))\n",
    "class_to_idx = {label: idx for idx, label in enumerate(class_labels)}\n",
    "print(\"Class labels:\", class_to_idx)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c683722f-bec9-417b-8f5d-49036fe4fdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Model Definition\n",
    "num_classes = len(class_labels)\n",
    "\n",
    "classification_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "num_features = classification_model.fc.in_features\n",
    "classification_model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classification_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445f044c-e2c2-4a6e-abab-386ec06ae016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 227\n",
      "Number of annotations: 227\n",
      "Number of images without annotations: 98\n",
      "Number of images: 53\n",
      "Number of annotations: 53\n",
      "Number of images without annotations: 41\n",
      "Number of images: 27\n",
      "Number of annotations: 27\n",
      "Number of images without annotations: 20\n",
      "Epoch [1/50], Loss: 1.3767, Accuracy: 0.2907\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'vehicle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_images, val_labels \u001b[38;5;129;01min\u001b[39;00m val_dataloader:\n\u001b[0;32m     72\u001b[0m     val_images \u001b[38;5;241m=\u001b[39m val_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 73\u001b[0m     val_labels \u001b[38;5;241m=\u001b[39m [class_to_idx[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m val_labels]\n\u001b[0;32m     74\u001b[0m     val_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(val_labels)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     76\u001b[0m     val_outputs \u001b[38;5;241m=\u001b[39m classification_model(val_images)\n",
      "Cell \u001b[1;32mIn[5], line 73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_images, val_labels \u001b[38;5;129;01min\u001b[39;00m val_dataloader:\n\u001b[0;32m     72\u001b[0m     val_images \u001b[38;5;241m=\u001b[39m val_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 73\u001b[0m     val_labels \u001b[38;5;241m=\u001b[39m [\u001b[43mclass_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m val_labels]\n\u001b[0;32m     74\u001b[0m     val_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(val_labels)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     76\u001b[0m     val_outputs \u001b[38;5;241m=\u001b[39m classification_model(val_images)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'vehicle'"
     ]
    }
   ],
   "source": [
    "# Cell 5: Image Classification Training\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "classification_optimizer = torch.optim.AdamW(classification_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(classification_optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Initialize variables for best validation accuracy and corresponding epoch\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "# Create data loaders for training, validation, and testing\n",
    "train_dataset = CustomDataset(dataset_path, split=\"train\", transform=transform)\n",
    "val_dataset = CustomDataset(dataset_path, split=\"val\", transform=transform)\n",
    "test_dataset = CustomDataset(dataset_path, split=\"test\", transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = [class_to_idx[label] for label in labels]\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = classification_model(images)\n",
    "        loss = classification_criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        classification_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        classification_optimizer.step()\n",
    "        \n",
    "        # Calculate running loss and accuracy\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        total_samples += images.size(0)\n",
    "    \n",
    "    # Calculate epoch loss and accuracy\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Perform validation\n",
    "    classification_model.eval()\n",
    "    \n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "    val_total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in val_dataloader:\n",
    "            val_images = val_images.to(device)\n",
    "            val_labels = [class_to_idx[label] for label in val_labels]\n",
    "            val_labels = torch.tensor(val_labels).to(device)\n",
    "            \n",
    "            val_outputs = classification_model(val_images)\n",
    "            val_loss = classification_criterion(val_outputs, val_labels)\n",
    "            \n",
    "            val_running_loss += val_loss.item() * val_images.size(0)\n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "            val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
    "            val_total_samples += val_images.size(0)\n",
    "    \n",
    "    # Calculate validation loss and accuracy\n",
    "    val_loss = val_running_loss / val_total_samples\n",
    "    val_acc = val_running_corrects.double() / val_total_samples\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Check if current epoch has the best validation accuracy\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(classification_model.state_dict(), \"best_classification_model.pth\")\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f} at Epoch {best_epoch}\")\n",
    "\n",
    "# Load the best model\n",
    "classification_model.load_state_dict(torch.load(\"best_classification_model.pth\"))\n",
    "\n",
    "# Perform testing\n",
    "classification_model.eval()\n",
    "\n",
    "test_running_corrects = 0\n",
    "test_total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_images, test_labels in test_dataloader:\n",
    "        test_images = test_images.to(device)\n",
    "        test_labels = [class_to_idx[label] for label in test_labels]\n",
    "        test_labels = torch.tensor(test_labels).to(device)\n",
    "        \n",
    "        test_outputs = classification_model(test_images)\n",
    "        _, test_preds = torch.max(test_outputs, 1)\n",
    "        test_running_corrects += torch.sum(test_preds == test_labels.data)\n",
    "        test_total_samples += test_images.size(0)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_acc = test_running_corrects.double() / test_total_samples\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3eda0b-3666-44fe-b7bc-4283f6a080ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test Image Classification Model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = models.resnet50()\n",
    "num_features = loaded_model.fc.in_features\n",
    "loaded_model.fc = nn.Linear(num_features, num_classes)\n",
    "loaded_model.load_state_dict(torch.load(\"classification_model.pth\"))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "# Prepare the test dataset\n",
    "test_dataset = CustomDataset(dataset_path, split=\"test\", transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = [class_to_idx[label] for label in labels]\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "        \n",
    "        outputs = loaded_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa58396-5d94-470d-a792-b65d87981f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualize Predictions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Get a random image from the test dataset\n",
    "test_dataset = CustomDataset(dataset_path, split=\"test\", transform=transform)\n",
    "random_index = random.randint(0, len(test_dataset) - 1)\n",
    "image, label = test_dataset[random_index]\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    output = loaded_model(image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    predicted_label = class_labels[predicted.item()]\n",
    "\n",
    "# Visualize the image and predicted label\n",
    "plt.imshow(image.squeeze().cpu().permute(1, 2, 0))\n",
    "plt.title(f\"Predicted: {predicted_label}, Actual: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7a513-04ad-4775-a0e7-597b5e4c39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Performance Metrics and Visualization\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = [class_to_idx[label] for label in labels]\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "        \n",
    "        outputs = loaded_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Binarize the labels for each class\n",
    "true_labels_binary = label_binarize(true_labels, classes=range(num_classes))\n",
    "predictions_binary = label_binarize(predictions, classes=range(num_classes))\n",
    "\n",
    "# Calculate precision-recall curve and average precision score for each class\n",
    "precision_curves = []\n",
    "recall_curves = []\n",
    "avg_precisions = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(true_labels_binary[:, i], predictions_binary[:, i])\n",
    "    precision_curves.append(precision_curve)\n",
    "    recall_curves.append(recall_curve)\n",
    "    avg_precision = average_precision_score(true_labels_binary[:, i], predictions_binary[:, i])\n",
    "    avg_precisions.append(avg_precision)\n",
    "\n",
    "# Calculate the mean average precision score across all classes\n",
    "mean_avg_precision = np.mean(avg_precisions)\n",
    "\n",
    "# Visualize the precision-recall curve for each class\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(num_classes):\n",
    "    plt.plot(recall_curves[i], precision_curves[i], label=f'Class {i}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Average Precision: {mean_avg_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3bc2d2-dd82-4754-abd4-7ac3c540b50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
