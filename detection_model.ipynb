{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4b4b2a-a2af-4860-9ce9-4985c5ac31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4bfbbfb-f266-4adb-b3ba-e5a30c41366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_path, split, image_type, transform=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.image_type = image_type\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "\n",
    "        images_dir = os.path.join(dataset_path, split, image_type, \"images\")\n",
    "        annotations_dir = os.path.join(dataset_path, split, image_type, \"annotations\")\n",
    "\n",
    "        for filename in os.listdir(images_dir):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(images_dir, filename)\n",
    "                self.images.append(image_path)\n",
    "\n",
    "                annotation_path = os.path.join(annotations_dir, os.path.splitext(filename)[0] + \".xml\")\n",
    "                try:\n",
    "                    tree = ET.parse(annotation_path)\n",
    "                    root = tree.getroot()\n",
    "                    annotation = []\n",
    "\n",
    "                    for obj in root.findall(\"object\"):\n",
    "                        name = obj.find(\"name\").text\n",
    "                        bbox = obj.find(\"bndbox\")\n",
    "                        xmin = int(bbox.find(\"xmin\").text)\n",
    "                        ymin = int(bbox.find(\"ymin\").text)\n",
    "                        xmax = int(bbox.find(\"xmax\").text)\n",
    "                        ymax = int(bbox.find(\"ymax\").text)\n",
    "                        annotation.append((name, xmin, ymin, xmax, ymax))\n",
    "\n",
    "                    self.annotations.append(annotation)\n",
    "                except ET.ParseError:\n",
    "                    print(f\"Error parsing annotation file: {annotation_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        try:\n",
    "            if self.image_type == \"rgb\":\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "            elif self.image_type == \"thermal\":\n",
    "                image = Image.open(image_path).convert(\"L\")  # Load thermal image as grayscale\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid image type: {self.image_type}\")\n",
    "\n",
    "            annotation = self.annotations[index]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            boxes = torch.as_tensor([ann[1:] for ann in annotation], dtype=torch.float32)\n",
    "            labels = torch.as_tensor([ann[0] for ann in annotation], dtype=torch.int64)\n",
    "\n",
    "            return image, {\"boxes\": boxes, \"labels\": labels}\n",
    "        except (IOError, ValueError):\n",
    "            print(f\"Error loading image file: {image_path}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fb77a3-8ced-43ec-9852-2dd8ce9ff503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Dataset Preprocessing\n",
    "def preprocess_dataset(dataset):\n",
    "    preprocessed_images = []\n",
    "    preprocessed_annotations = []\n",
    "    \n",
    "    for image, annotation in dataset:\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = transforms.ToPILImage()(image)\n",
    "        else:\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        image = transform(image)\n",
    "        \n",
    "        targets = []\n",
    "        for obj in annotation:\n",
    "            name, xmin, ymin, xmax, ymax = obj\n",
    "            label = class_to_idx[name]\n",
    "            xmin, ymin, xmax, ymax = xmin / image.shape[2], ymin / image.shape[1], xmax / image.shape[2], ymax / image.shape[1]\n",
    "            targets.append([label, xmin, ymin, xmax, ymax])\n",
    "        targets = torch.tensor(targets)\n",
    "        \n",
    "        preprocessed_images.append(image)\n",
    "        preprocessed_annotations.append(targets)\n",
    "    \n",
    "    return preprocessed_images, preprocessed_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a351344-9bb1-4856-93c0-a3eaf54885e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/train/rgb/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m      5\u001b[0m rgb_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m800\u001b[39m)),  \n\u001b[1;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(\u001b[38;5;241m0.5\u001b[39m),  \n\u001b[1;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),  \n\u001b[1;32m      9\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \n\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     12\u001b[0m thermal_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m800\u001b[39m)),  \n\u001b[1;32m     14\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(\u001b[38;5;241m0.5\u001b[39m),  \n\u001b[1;32m     15\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),  \n\u001b[1;32m     16\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m])  \n\u001b[1;32m     17\u001b[0m ])\n\u001b[0;32m---> 19\u001b[0m rgb_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgb_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m thermal_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(dataset_path, split, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthermal\u001b[39m\u001b[38;5;124m\"\u001b[39m, thermal_transform)\n\u001b[1;32m     22\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, dataset_path, split, image_type, transform)\u001b[0m\n\u001b[1;32m     11\u001b[0m images_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, split, image_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m annotations_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, split, image_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     16\u001b[0m         image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(images_dir, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/train/rgb/images'"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataset and DataLoader Creation\n",
    "dataset_path = \"dataset\"\n",
    "split = \"train\"\n",
    "\n",
    "rgb_transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),  \n",
    "    transforms.RandomHorizontalFlip(0.5),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "thermal_transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),  \n",
    "    transforms.RandomHorizontalFlip(0.5),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "])\n",
    "\n",
    "rgb_dataset = CustomDataset(dataset_path, split, \"rgb\", rgb_transform)\n",
    "thermal_dataset = CustomDataset(dataset_path, split, \"thermal\", thermal_transform)\n",
    "\n",
    "class_labels = set()\n",
    "for _, annotation in rgb_dataset:\n",
    "    for obj in annotation:\n",
    "        name = obj[0]\n",
    "        class_labels.add(name)\n",
    "\n",
    "class_to_idx = {label: idx for idx, label in enumerate(class_labels)}\n",
    "print(\"Class labels:\", class_to_idx)\n",
    "\n",
    "preprocessed_rgb_images, preprocessed_rgb_annotations = preprocess_dataset(rgb_dataset)\n",
    "preprocessed_thermal_images, preprocessed_thermal_annotations = preprocess_dataset(thermal_dataset)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_rgb_dataset = list(zip(preprocessed_rgb_images, preprocessed_rgb_annotations))\n",
    "train_thermal_dataset = list(zip(preprocessed_thermal_images, preprocessed_thermal_annotations))\n",
    "\n",
    "train_rgb_loader = DataLoader(train_rgb_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "train_thermal_loader = DataLoader(train_thermal_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62116b6a-7c4f-4361-b539-0f1d531c7386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4418\n",
      "Epoch [2/10], Loss: 0.3149\n",
      "Epoch [3/10], Loss: 0.2285\n",
      "Epoch [4/10], Loss: 0.3032\n",
      "Epoch [5/10], Loss: 0.2469\n",
      "Epoch [6/10], Loss: 0.1760\n",
      "Epoch [7/10], Loss: 0.2289\n",
      "Epoch [8/10], Loss: 0.2086\n",
      "Epoch [9/10], Loss: 0.1681\n",
      "Epoch [10/10], Loss: 0.1353\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Model Definition and Training\n",
    "num_classes = len(class_to_idx) + 1\n",
    "\n",
    "rgb_model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = rgb_model.roi_heads.box_predictor.cls_score.in_features\n",
    "rgb_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "thermal_model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = thermal_model.roi_heads.box_predictor.cls_score.in_features\n",
    "thermal_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rgb_model.to(device)\n",
    "thermal_model.to(device)\n",
    "\n",
    "rgb_optimizer = torch.optim.SGD(rgb_model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "thermal_optimizer = torch.optim.SGD(thermal_model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    rgb_model.train()\n",
    "    thermal_model.train()\n",
    "    \n",
    "    rgb_epoch_loss = 0.0\n",
    "    thermal_epoch_loss = 0.0\n",
    "    \n",
    "    for rgb_images, rgb_targets in train_rgb_loader:\n",
    "        rgb_images = list(image.to(device) for image in rgb_images)\n",
    "        rgb_targets = [{'boxes': t[:, 1:], 'labels': t[:, 0].long()} for t in rgb_targets]\n",
    "        \n",
    "        rgb_loss_dict = rgb_model(rgb_images, rgb_targets)\n",
    "        rgb_losses = sum(loss for loss in rgb_loss_dict.values())\n",
    "        \n",
    "        rgb_optimizer.zero_grad()\n",
    "        rgb_losses.backward()\n",
    "        rgb_optimizer.step()\n",
    "        \n",
    "        rgb_epoch_loss += rgb_losses.item()\n",
    "    \n",
    "    for thermal_images, thermal_targets in train_thermal_loader:\n",
    "        thermal_images = list(image.to(device) for image in thermal_images)\n",
    "        thermal_targets = [{'boxes': t[:, 1:], 'labels': t[:, 0].long()} for t in thermal_targets]\n",
    "        \n",
    "        thermal_loss_dict = thermal_model(thermal_images, thermal_targets)\n",
    "        thermal_losses = sum(loss for loss in thermal_loss_dict.values())\n",
    "        \n",
    "        thermal_optimizer.zero_grad()\n",
    "        thermal_losses.backward()\n",
    "        thermal_optimizer.step()\n",
    "        \n",
    "        thermal_epoch_loss += thermal_losses.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], RGB Loss: {rgb_epoch_loss/len(train_rgb_loader):.4f}, Thermal Loss: {thermal_epoch_loss/len(train_thermal_loader):.4f}\")\n",
    "\n",
    "torch.save(rgb_model.state_dict(), \"rgb_trained_model.pth\")\n",
    "torch.save(thermal_model.state_dict(), \"thermal_trained_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce10856c-392b-4009-b458-7c4553fd80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Evaluation and Testing\n",
    "rgb_model.eval()\n",
    "thermal_model.eval()\n",
    "\n",
    "test_rgb_dataset = CustomDataset(dataset_path, split=\"test\", image_type=\"rgb\", transform=rgb_transform)\n",
    "test_thermal_dataset = CustomDataset(dataset_path, split=\"test\", image_type=\"thermal\", transform=thermal_transform)\n",
    "\n",
    "test_rgb_images, test_rgb_annotations = preprocess_dataset(test_rgb_dataset)\n",
    "test_thermal_images, test_thermal_annotations = preprocess_dataset(test_thermal_dataset)\n",
    "\n",
    "test_rgb_dataset = list(zip(test_rgb_images, test_rgb_annotations))\n",
    "test_thermal_dataset = list(zip(test_thermal_images, test_thermal_annotations))\n",
    "\n",
    "test_rgb_loader = DataLoader(test_rgb_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "test_thermal_loader = DataLoader(test_thermal_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for rgb_image, rgb_target in test_rgb_loader:\n",
    "    rgb_image = rgb_image[0].unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    with torch.no_grad():\n",
    "        rgb_predictions = rgb_model(rgb_image)\n",
    "    \n",
    "    rgb_boxes = rgb_predictions[0]['boxes'].cpu().numpy()\n",
    "    rgb_labels = rgb_predictions[0]['labels'].cpu().numpy()\n",
    "    rgb_scores = rgb_predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    rgb_confidence_threshold = 0.5\n",
    "    rgb_mask = rgb_scores >= rgb_confidence_threshold\n",
    "    rgb_boxes = rgb_boxes[rgb_mask]\n",
    "    rgb_labels = rgb_labels[rgb_mask]\n",
    "    rgb_scores = rgb_scores[rgb_mask]\n",
    "    \n",
    "    rgb_image = rgb_image.squeeze(0).cpu().numpy().transpose((1, 2, 0))  # Remove batch dimension and change shape to [H, W, C]\n",
    "    rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    for box, label, score in zip(rgb_boxes, rgb_labels, rgb_scores):\n",
    "        xmin, ymin, xmax, ymax = box.astype(int)\n",
    "        label_name = list(class_to_idx.keys())[list(class_to_idx.values()).index(label)]\n",
    "        \n",
    "        cv2.rectangle(rgb_image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(rgb_image, f\"{label_name}: {score:.2f}\", (xmin, ymin - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"RGB Object Detection\", rgb_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "for thermal_image, thermal_target in test_thermal_loader:\n",
    "    thermal_image = thermal_image[0].unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    with torch.no_grad():\n",
    "        thermal_predictions = thermal_model(thermal_image)\n",
    "    \n",
    "    thermal_boxes = thermal_predictions[0]['boxes'].cpu().numpy()\n",
    "    thermal_labels = thermal_predictions[0]['labels'].cpu().numpy()\n",
    "    thermal_scores = thermal_predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    thermal_confidence_threshold = 0.5\n",
    "    thermal_mask = thermal_scores >= thermal_confidence_threshold\n",
    "    thermal_boxes = thermal_boxes[thermal_mask]\n",
    "    thermal_labels = thermal_labels[thermal_mask]\n",
    "    thermal_scores = thermal_scores[thermal_mask]\n",
    "    \n",
    "    thermal_image = thermal_image.squeeze(0).cpu().numpy().transpose((1, 2, 0))  # Remove batch dimension and change shape to [H, W]\n",
    "    thermal_image = cv2.normalize(thermal_image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    thermal_image = cv2.applyColorMap(thermal_image, cv2.COLORMAP_JET)\n",
    "    \n",
    "    for box, label, score in zip(thermal_boxes, thermal_labels, thermal_scores):\n",
    "        xmin, ymin, xmax, ymax = box.astype(int)\n",
    "        label_name = list(class_to_idx.keys())[list(class_to_idx.values()).index(label)]\n",
    "        \n",
    "        cv2.rectangle(thermal_image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(thermal_image, f\"{label_name}: {score:.2f}\", (xmin, ymin - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Thermal Object Detection\", thermal_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd00703-0256-43c0-b7e0-dfd12bda7381",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Video Object Detection\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m input_video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbridge_1.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m input_video \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(input_video_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 7: Video Slicing into Frames\n",
    "def slice_video_into_frames(video_path, output_dir):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_path = os.path.join(output_dir, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return fps\n",
    "\n",
    "input_rgb_video_path = \"rgb_video.mp4\"\n",
    "input_thermal_video_path = \"thermal_video.mp4\"\n",
    "\n",
    "output_rgb_frames_dir = \"rgb_frames\"\n",
    "output_thermal_frames_dir = \"thermal_frames\"\n",
    "\n",
    "rgb_fps = slice_video_into_frames(input_rgb_video_path, output_rgb_frames_dir)\n",
    "thermal_fps = slice_video_into_frames(input_thermal_video_path, output_thermal_frames_dir)\n",
    "\n",
    "print(f\"RGB video sliced into frames at {rgb_fps} FPS. Frames saved in {output_rgb_frames_dir}.\")\n",
    "print(f\"Thermal video sliced into frames at {thermal_fps} FPS. Frames saved in {output_thermal_frames_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ca309-781e-4ea9-8194-c8348a9f6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_video_into_frames(video_path, output_dir):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_path = os.path.join(output_dir, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return fps\n",
    "\n",
    "input_video_path = \"\"\n",
    "\n",
    "output_frames_dir = \"\"\n",
    "\n",
    "fps = slice_video_into_frames(input_video_path, output_frames_dir)\n",
    "\n",
    "print(f\"Video sliced into frames at {fps} FPS. Frames saved in {output_frames_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11042594-fc1f-428a-a464-70a7445127ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m frames_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     22\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     23\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m800\u001b[39m)),\n\u001b[1;32m     24\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     25\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     26\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_frame(frame_path, transform):\n",
    "    frame = Image.open(frame_path).convert(\"RGB\")\n",
    "    preprocessed_frame = transform(frame)\n",
    "    return preprocessed_frame\n",
    "\n",
    "def detect_objects(model, frame_path, transform, device):\n",
    "    preprocessed_frame = preprocess_frame(frame_path, transform)\n",
    "    input_tensor = preprocessed_frame.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_tensor)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "frames_dir = \"\"\n",
    "\n",
    "model_path = \"\"\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for frame_name in os.listdir(frames_dir):\n",
    "    frame_path = os.path.join(frames_dir, frame_name)\n",
    "    predictions = detect_objects(model, frame_path, transform, device)\n",
    "    print(f\"Processed frame: {frame_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0792e354-99dd-493e-809f-214b7d877b9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m vehicle_counts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m vehicle_locations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(frames_dir):\n\u001b[1;32m     20\u001b[0m     frame_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(frames_dir, frame_name)\n\u001b[1;32m     21\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m detect_objects(model, frame_path, transform, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def count_vehicles(predictions, confidence_threshold=0.5):\n",
    "    vehicle_count = 0\n",
    "    vehicle_boxes = []\n",
    "\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        if score >= confidence_threshold and label == 1:  \n",
    "            vehicle_count += 1\n",
    "            vehicle_boxes.append(box)\n",
    "\n",
    "    return vehicle_count, vehicle_boxes\n",
    "\n",
    "vehicle_counts = []\n",
    "vehicle_locations = []\n",
    "\n",
    "for frame_name in os.listdir(frames_dir):\n",
    "    frame_path = os.path.join(frames_dir, frame_name)\n",
    "    predictions = detect_objects(model, frame_path, transform, device)\n",
    "\n",
    "    count, boxes = count_vehicles(predictions, confidence_threshold=0.5)\n",
    "    vehicle_counts.append(count)\n",
    "    vehicle_locations.append(boxes)\n",
    "\n",
    "    print(f\"Processed frame: {frame_name}, Vehicle count: {count}\")\n",
    "\n",
    "total_vehicles = sum(vehicle_counts)\n",
    "print(f\"Total vehicles detected: {total_vehicles}\")\n",
    "print(\"Vehicle locations:\")\n",
    "for i, (count, boxes) in enumerate(zip(vehicle_counts, vehicle_locations)):\n",
    "    print(f\"Frame {i}: Count: {count}, Boxes: {boxes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f84a5a2-73cb-4842-804d-b900bdeb8667",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vehicle_images\n\u001b[1;32m     14\u001b[0m vehicle_images_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(vehicle_images_dir):\n\u001b[1;32m     17\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(vehicle_images_dir)\n\u001b[1;32m     19\u001b[0m vehicle_image_paths \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_vehicle_images(frame_path, vehicle_boxes, output_dir):\n",
    "    frame = cv2.imread(frame_path)\n",
    "    vehicle_images = []\n",
    "\n",
    "    for i, box in enumerate(vehicle_boxes):\n",
    "        xmin, ymin, xmax, ymax = box.astype(int)\n",
    "        vehicle_image = frame[ymin:ymax, xmin:xmax]\n",
    "        vehicle_image_path = os.path.join(output_dir, f\"vehicle_{i}.jpg\")\n",
    "        cv2.imwrite(vehicle_image_path, vehicle_image)\n",
    "        vehicle_images.append(vehicle_image_path)\n",
    "\n",
    "    return vehicle_images\n",
    "\n",
    "vehicle_images_dir = \"\"\n",
    "\n",
    "if not os.path.exists(vehicle_images_dir):\n",
    "    os.makedirs(vehicle_images_dir)\n",
    "\n",
    "vehicle_image_paths = []\n",
    "\n",
    "for frame_name, boxes in zip(os.listdir(frames_dir), vehicle_locations):\n",
    "    frame_path = os.path.join(frames_dir, frame_name)\n",
    "    vehicle_images = extract_vehicle_images(frame_path, boxes, vehicle_images_dir)\n",
    "    vehicle_image_paths.extend(vehicle_images)\n",
    "\n",
    "    print(f\"Extracted vehicle images for frame: {frame_name}\")\n",
    "\n",
    "print(\"Extracted vehicle images:\")\n",
    "for path in vehicle_image_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d30416-5988-4880-9795-41b975c41d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m         _, predicted_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_class\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 16\u001b[0m resnet_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet50(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Assuming 4 vehicle classes: car, truck, bus, motorcycle\u001b[39;00m\n\u001b[1;32m     18\u001b[0m num_features \u001b[38;5;241m=\u001b[39m resnet_model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_vehicle_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed_image = transform(image)\n",
    "    return preprocessed_image\n",
    "\n",
    "def classify_vehicle(model, image_path, transform, device):\n",
    "    preprocessed_image = preprocess_vehicle_image(image_path, transform)\n",
    "    input_tensor = preprocessed_image.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        _, predicted_class = torch.max(outputs, 1)\n",
    "\n",
    "    return predicted_class.item()\n",
    "\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "num_classes = 4  \n",
    "num_features = resnet_model.fc.in_features\n",
    "resnet_model.fc = torch.nn.Linear(num_features, num_classes)\n",
    "resnet_model.to(device)\n",
    "resnet_model.eval()\n",
    "\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "vehicle_classifications = []\n",
    "\n",
    "for image_path in vehicle_image_paths:\n",
    "    predicted_class = classify_vehicle(resnet_model, image_path, resnet_transform, device)\n",
    "    vehicle_classifications.append(predicted_class)\n",
    "\n",
    "    print(f\"Classified vehicle: {image_path}, Class: {predicted_class}\")\n",
    "\n",
    "print(\"Vehicle classifications using ResNet:\")\n",
    "for image_path, classification in zip(vehicle_image_paths, vehicle_classifications):\n",
    "    print(f\"Image: {image_path}, Class: {classification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933baca1-8f09-484e-b026-fa68ef4be801",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Precision, Recall, F1Score, Accuracy\n\u001b[1;32m      3\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlargeCar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmallCar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheavyTruck\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightTruck\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m [class_labels[\u001b[38;5;28mcls\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vehicle_classifications]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Precision, Recall, F1Score, Accuracy\n",
    "\n",
    "class_labels = {\n",
    "    0: 'largeCar',\n",
    "    1: 'smallCar',\n",
    "    2: 'heavyTruck',\n",
    "    3: 'lightTruck'\n",
    "}\n",
    "\n",
    "predicted_labels = [class_labels[cls] for cls in vehicle_classifications]\n",
    "\n",
    "ground_truth_labels = [\n",
    "    'largeCar',\n",
    "    'smallCar',\n",
    "    'heavyTruck',\n",
    "    'lightTruck',\n",
    "    'smallCar',\n",
    "    'largeCar',\n",
    "    'heavyTruck',\n",
    "    'largeCar',\n",
    "    'lightTruck',\n",
    "    'smallCar'\n",
    "]\n",
    "\n",
    "ground_truth_indices = [list(class_labels.values()).index(label) for label in ground_truth_labels]\n",
    "\n",
    "predicted_labels_tensor = torch.tensor(vehicle_classifications)\n",
    "ground_truth_indices_tensor = torch.tensor(ground_truth_indices)\n",
    "\n",
    "precision = Precision(num_classes=len(class_labels), average='macro')(predicted_labels_tensor, ground_truth_indices_tensor)\n",
    "recall = Recall(num_classes=len(class_labels), average='macro')(predicted_labels_tensor, ground_truth_indices_tensor)\n",
    "f1_score = F1Score(num_classes=len(class_labels), average='macro')(predicted_labels_tensor, ground_truth_indices_tensor)\n",
    "accuracy = Accuracy()(predicted_labels_tensor, ground_truth_indices_tensor)\n",
    "\n",
    "print(\"Vehicle Classification Evaluation:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "574071eb-277c-4dda-84f4-edb645b2b94e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m processed_frames_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m labeled_frames_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(labeled_frames_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_name, boxes, labels, scores \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(frames_dir), vehicle_locations, vehicle_classifications, vehicle_scores):\n\u001b[1;32m     19\u001b[0m     frame_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(frames_dir, frame_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def draw_bounding_boxes(image, boxes, labels, scores, class_labels, confidence_threshold=0.5):\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= confidence_threshold:\n",
    "            xmin, ymin, xmax, ymax = box.astype(int)\n",
    "            class_name = class_labels[label]\n",
    "            \n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f\"{class_name}: {score:.2f}\", (xmin, ymin - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "processed_frames_dir = \"\"\n",
    "\n",
    "labeled_frames_dir = \"\"\n",
    "os.makedirs(labeled_frames_dir, exist_ok=True)\n",
    "\n",
    "for frame_name, boxes, labels, scores in zip(os.listdir(frames_dir), vehicle_locations, vehicle_classifications, vehicle_scores):\n",
    "    frame_path = os.path.join(frames_dir, frame_name)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    \n",
    "    labeled_frame = draw_bounding_boxes(frame, boxes, labels, scores, class_labels)\n",
    "    \n",
    "    labeled_frame_path = os.path.join(labeled_frames_dir, frame_name)\n",
    "    cv2.imwrite(labeled_frame_path, labeled_frame)\n",
    "    \n",
    "    print(f\"Labeled frame: {frame_name}\")\n",
    "\n",
    "print(\"Labeling completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877d189-6b96-4c0b-9ca0-1a9638635858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_video_from_frames(frames_dir, output_path, fps):\n",
    "    frame_files = sorted(os.listdir(frames_dir), key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "    \n",
    "    if len(frame_files) == 0:\n",
    "        print(\"No frames found in the directory.\")\n",
    "        return\n",
    "    \n",
    "    first_frame_path = os.path.join(frames_dir, frame_files[0])\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    height, width, _ = first_frame.shape\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(frames_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        video_writer.write(frame)\n",
    "    \n",
    "    video_writer.release()\n",
    "    \n",
    "    print(\"Video reconstruction completed.\")\n",
    "\n",
    "labeled_frames_dir = \"\"\n",
    "\n",
    "output_video_path = \"\"\n",
    "\n",
    "output_fps = 30\n",
    "\n",
    "rebuild_video_from_frames(labeled_frames_dir, output_video_path, output_fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
