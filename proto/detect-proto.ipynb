{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b528f-67f8-479a-bbdf-192f9e13dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed462c-91e2-4f8b-9890-23be5d1b1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\", transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        self.image_annotation_count = 0\n",
    "        self.no_annotation_count = 0\n",
    "\n",
    "        images_dir = os.path.join(root_dir, \"raw-images\")\n",
    "        annotations_dir = os.path.join(root_dir, \"annotations\")\n",
    "\n",
    "        for subfolder in os.listdir(images_dir):\n",
    "            subfolder_images_dir = os.path.join(images_dir, subfolder)\n",
    "            subfolder_annotations_dir = os.path.join(annotations_dir, subfolder)\n",
    "\n",
    "            if not os.path.isdir(subfolder_annotations_dir):\n",
    "                continue\n",
    "\n",
    "            image_files = [f for f in os.listdir(subfolder_images_dir) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "            \n",
    "            # Filter image files to include only those with \"rgb\" in the filename\n",
    "            image_files = [f for f in image_files if \"rgb\" in f.lower()]\n",
    "            image_files.sort()\n",
    "\n",
    "            num_images = len(image_files)\n",
    "            if split == \"train\":\n",
    "                image_files = image_files[:int(0.7 * num_images)]\n",
    "            elif split == \"val\":\n",
    "                image_files = image_files[int(0.7 * num_images):int(0.9 * num_images)]\n",
    "            elif split == \"test\":\n",
    "                image_files = image_files[int(0.9 * num_images):]\n",
    "\n",
    "            for filename in image_files:\n",
    "                image_path = os.path.join(subfolder_images_dir, filename)\n",
    "                annotation_path = os.path.join(subfolder_annotations_dir, os.path.splitext(filename)[0] + \".xml\")\n",
    "\n",
    "                if os.path.exists(annotation_path):\n",
    "                    tree = ET.parse(annotation_path)\n",
    "                    root = tree.getroot()\n",
    "                    annotation = []\n",
    "\n",
    "                    for obj in root.findall(\"object\"):\n",
    "                        name = obj.find(\"name\").text\n",
    "                        bbox = obj.find(\"bndbox\")\n",
    "                        xmin = int(bbox.find(\"xmin\").text)\n",
    "                        ymin = int(bbox.find(\"ymin\").text)\n",
    "                        xmax = int(bbox.find(\"xmax\").text)\n",
    "                        ymax = int(bbox.find(\"ymax\").text)\n",
    "                        annotation.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "                    if len(annotation) > 0:\n",
    "                        self.images.append(image_path)\n",
    "                        self.annotations.append(annotation)\n",
    "                        self.image_annotation_count += 1\n",
    "                    else:\n",
    "                        self.no_annotation_count += 1\n",
    "                else:\n",
    "                    self.no_annotation_count += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Ensure image is in RGB format\n",
    "        annotations = self.annotations[index]\n",
    "\n",
    "        boxes = torch.as_tensor(annotations, dtype=torch.float32)\n",
    "        labels = torch.ones((len(annotations),), dtype=torch.int64)  # Assuming all objects are 'vehicle'\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8803b-ad51-490f-a357-b248ab6b1749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Test the dataset\n",
    "dataset_path = \"dataset\"\n",
    "split = \"train\"\n",
    "\n",
    "thermal_transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "thermal_dataset = CustomDataset(dataset_path, split, thermal_transform)\n",
    "print(f\"Number of images in the dataset: {len(thermal_dataset)}\")\n",
    "image, target, image_path = thermal_dataset[0]\n",
    "print(f\"Image path: {image_path}\")\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Target boxes: {target['boxes']}\")\n",
    "print(f\"Target labels: {target['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f6551-03b4-49ce-aa75-28c340899847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    preprocessed_images = []\n",
    "    preprocessed_annotations = []\n",
    "    \n",
    "    for image, target in dataset:\n",
    "        # Ensure image is a PIL Image; this should already be the case\n",
    "        if not isinstance(image, Image.Image):\n",
    "            raise TypeError(\"The dataset must return PIL Image objects.\")\n",
    "        \n",
    "        # Apply transformations\n",
    "        image = rgb_transform(image)\n",
    "        \n",
    "        # Update target 'boxes' format if necessary\n",
    "        boxes = target['boxes']\n",
    "        labels = target['labels']\n",
    "\n",
    "        # Normalize bounding box coordinates\n",
    "        width, height = image.size  # Accessing size property of PIL Image\n",
    "        boxes[:, [0, 2]] = boxes[:, [0, 2]] / width\n",
    "        boxes[:, [1, 3]] = boxes[:, [1, 3]] / height\n",
    "        \n",
    "        # Update target dictionary\n",
    "        target = {'boxes': boxes, 'labels': labels}\n",
    "        \n",
    "        preprocessed_images.append(image)\n",
    "        preprocessed_annotations.append(target)\n",
    "    \n",
    "    return preprocessed_images, preprocessed_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba4709a-4bfd-47cb-b4fb-13313163cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Dataset and DataLoader Creation\n",
    "rgb_transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "rgb_dataset = CustomDataset(dataset_path, split, rgb_transform)\n",
    "\n",
    "class_labels = set()\n",
    "for _, target, _ in rgb_dataset:\n",
    "    for obj in target[\"labels\"]:\n",
    "        class_labels.add(\"vehicle\")  \n",
    "\n",
    "class_to_idx = {\"vehicle\": 0}\n",
    "print(\"Class labels:\", class_to_idx)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    image_paths = [item[2] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets, image_paths\n",
    "\n",
    "train_rgb_loader = DataLoader(rgb_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for images, targets, image_paths in train_rgb_loader:\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Batch targets boxes shape: {targets[0]['boxes'].shape}\")\n",
    "    print(f\"Batch targets labels shape: {targets[0]['labels'].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6623a-9064-4031-ac93-d852c0ddb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Model Training\n",
    "coco_vehicle_ids = [3, 8, 6, 4]  \n",
    "coco_vehicle_labels = ['car', 'truck', 'bus', 'motorcycle']\n",
    "\n",
    "dataset_label_to_coco_id = {'vehicle': coco_vehicle_ids}  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_classes = len(coco_vehicle_labels) + 1  \n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.roi_heads.box_predictor.to(device)  \n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1_scores = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, _ in dataloader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for output, target in zip(outputs, targets):\n",
    "                boxes = output['boxes'].cpu().numpy()\n",
    "                labels = output['labels'].cpu().numpy()\n",
    "                scores = output['scores'].cpu().numpy()\n",
    "                \n",
    "                # Apply non-maximum suppression to remove overlapping detections\n",
    "                indices = torchvision.ops.nms(torch.tensor(boxes), torch.tensor(scores), iou_threshold=0.5)\n",
    "                \n",
    "                predicted_labels = labels[indices]\n",
    "                predicted_boxes = boxes[indices]\n",
    "                predicted_scores = scores[indices]\n",
    "                \n",
    "                if isinstance(predicted_labels, np.int64):\n",
    "                    predicted_labels = [predicted_labels]\n",
    "                if isinstance(predicted_boxes, np.ndarray) and predicted_boxes.ndim == 1:\n",
    "                    predicted_boxes = [predicted_boxes]\n",
    "                if isinstance(predicted_scores, np.float32):\n",
    "                    predicted_scores = [predicted_scores]\n",
    "                \n",
    "                target_labels = target['labels'].cpu().numpy()\n",
    "                target_boxes = target['boxes'].cpu().numpy()\n",
    "                \n",
    "                if isinstance(target_labels, np.int64):\n",
    "                    target_labels = [target_labels]\n",
    "                if isinstance(target_boxes, np.ndarray) and target_boxes.ndim == 1:\n",
    "                    target_boxes = [target_boxes]\n",
    "                \n",
    "                if len(predicted_labels) == 0 or len(target_labels) == 0:\n",
    "                    continue\n",
    "                \n",
    "                if len(predicted_labels) < len(target_labels):\n",
    "                    predicted_labels = np.pad(predicted_labels, (0, len(target_labels) - len(predicted_labels)), mode='constant')\n",
    "                elif len(predicted_labels) > len(target_labels):\n",
    "                    predicted_labels = predicted_labels[:len(target_labels)]\n",
    "                \n",
    "                precision = precision_score(y_true=target_labels, y_pred=predicted_labels, average='weighted', zero_division=1)\n",
    "                recall = recall_score(y_true=target_labels, y_pred=predicted_labels, average='weighted', zero_division=1)\n",
    "                f1 = f1_score(y_true=target_labels, y_pred=predicted_labels, average='weighted', zero_division=1)\n",
    "                \n",
    "                all_precisions.append(precision)\n",
    "                all_recalls.append(recall)\n",
    "                all_f1_scores.append(f1)\n",
    "                all_predictions.append((predicted_boxes, predicted_labels, predicted_scores))\n",
    "                all_targets.append((target_boxes, target_labels))\n",
    "    \n",
    "    avg_precision = np.mean(all_precisions) if len(all_precisions) > 0 else 0.0\n",
    "    avg_recall = np.mean(all_recalls) if len(all_recalls) > 0 else 0.0\n",
    "    avg_f1 = np.mean(all_f1_scores) if len(all_f1_scores) > 0 else 0.0\n",
    "    \n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1-score: {avg_f1:.4f}\")\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "val_split = \"val\"\n",
    "val_rgb_dataset = CustomDataset(dataset_path, val_split, rgb_transform)\n",
    "val_rgb_loader = DataLoader(val_rgb_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "lr_values = [0.001, 0.0005, 0.0001]\n",
    "momentum_values = [0.9, 0.8]\n",
    "weight_decay_values = [0.0005, 0.0001]\n",
    "best_accuracy = 0.0\n",
    "best_model_state_dict = None\n",
    "\n",
    "for lr in lr_values:\n",
    "    for momentum in momentum_values:\n",
    "        for weight_decay in weight_decay_values:\n",
    "            optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Adjust the learning rate every 5 epochs\n",
    "\n",
    "            # Train the model\n",
    "            num_epochs = 10\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                epoch_loss = 0.0\n",
    "                for images, targets, _ in train_rgb_loader:\n",
    "                    images = list(img.to(device) for img in images)\n",
    "                    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                    loss_dict = model(images, targets)\n",
    "                    losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    losses.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += losses.item()\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_rgb_loader):.4f}\")\n",
    "\n",
    "                lr_scheduler.step()  \n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                avg_precision, avg_recall, avg_f1 = evaluate_model(model, val_rgb_loader, device)\n",
    "\n",
    "            if avg_f1 > best_accuracy:\n",
    "                best_accuracy = avg_f1\n",
    "                best_model_state_dict = model.state_dict()\n",
    "\n",
    "            print(f\"Hyperparameters: lr={lr}, momentum={momentum}, weight_decay={weight_decay}\")\n",
    "            print(f\"Validation Accuracy: {avg_f1:.4f}\\n\")\n",
    "\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "torch.save(model.state_dict(), \"trained_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b54a6a-d4d1-4a78-ae65-202639ddfcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Evaluation and Testing\n",
    "test_split = \"test\"\n",
    "test_rgb_dataset = CustomDataset(dataset_path, test_split, rgb_transform)\n",
    "test_rgb_loader = DataLoader(test_rgb_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "avg_precision, avg_recall, avg_f1 = evaluate_model(model, test_rgb_loader, device)\n",
    "\n",
    "print(f\"Number of images without annotations: {test_rgb_dataset.no_annotation_count}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1-score: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba4cbe-2489-43b6-8927-980eabef9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load the trained model\n",
    "model.load_state_dict(torch.load(\"trained_model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26e762-2ad1-4d30-aabe-95c025616211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Prepare the test dataset\n",
    "test_split = \"test\"\n",
    "test_rgb_dataset = CustomDataset(dataset_path, test_split, rgb_transform)\n",
    "test_rgb_loader = DataLoader(test_rgb_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280ef03-8e8f-4743-b14b-477da8ea1db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluate the model on the test dataset\n",
    "labels = [label.item() for _, target, _ in test_rgb_dataset for label in target['labels']]\n",
    "\n",
    "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "print(\"Label counts:\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254214f9-4be4-4092-85c0-5e9b058d3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "class_labels[1] = class_labels.get(1, 'vehicle')  \n",
    "\n",
    "def visualize_detections(image, boxes, labels, scores, class_labels, confidence_threshold=0.0):\n",
    "    image_with_detections = image.copy()\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= confidence_threshold:\n",
    "            xmin, ymin, xmax, ymax = box.astype(int)\n",
    "            class_name = class_labels.get(label, \"Unknown\")\n",
    "            cv2.rectangle(image_with_detections, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            cv2.putText(image_with_detections, f\"{class_name}: {score:.2f}\", (xmin, ymin - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    return image_with_detections\n",
    "\n",
    "for i in range(num_visualizations):\n",
    "    image_path = test_rgb_dataset.images[i]\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    image_tensor = rgb_transform(Image.fromarray(image)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "\n",
    "    boxes = outputs[0]['boxes'].cpu().numpy()\n",
    "    labels = outputs[0]['labels'].cpu().numpy()\n",
    "    scores = outputs[0]['scores'].cpu().numpy()\n",
    "\n",
    "    print(f\"Image {i+1} Path: {image_path}\")\n",
    "    print(f\"Boxes: {boxes}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(f\"Scores: {scores}\")\n",
    "\n",
    "    image_with_detections = visualize_detections(image, boxes, labels, scores, class_labels)\n",
    "\n",
    "    image_with_detections_rgb = cv2.cvtColor(image_with_detections, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image_with_detections_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539f537-3fdb-4293-846b-87fb33b28e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
