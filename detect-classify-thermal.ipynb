{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "165b528f-67f8-479a-bbdf-192f9e13dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU 0: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49ed462c-91e2-4f8b-9890-23be5d1b1131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the dataset: 297\n",
      "Image shape: torch.Size([1, 800, 800])\n",
      "Target boxes: tensor([[367., 137., 407., 153.],\n",
      "        [238., 125., 279., 145.],\n",
      "        [ 95., 131., 135., 144.],\n",
      "        [578., 136., 607., 149.]])\n",
      "Target labels: tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\", transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "\n",
    "        images_dir = os.path.join(root_dir, \"images\")\n",
    "        annotations_dir = os.path.join(root_dir, \"annotations\")\n",
    "\n",
    "        image_files = [f for f in os.listdir(images_dir) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "\n",
    "        image_files.sort()\n",
    "\n",
    "        num_images = len(image_files)\n",
    "        if split == \"train\":\n",
    "            image_files = image_files[:int(0.7 * num_images)]\n",
    "        elif split == \"val\":\n",
    "            image_files = image_files[int(0.7 * num_images):int(0.9 * num_images)]\n",
    "        elif split == \"test\":\n",
    "            image_files = image_files[int(0.9 * num_images):]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}\")\n",
    "\n",
    "        for filename in image_files:\n",
    "            image_path = os.path.join(images_dir, filename)\n",
    "            self.images.append(image_path)\n",
    "\n",
    "            annotation_path = os.path.join(annotations_dir, os.path.splitext(filename)[0] + \".xml\")\n",
    "            if os.path.exists(annotation_path):\n",
    "                tree = ET.parse(annotation_path)\n",
    "                root = tree.getroot()\n",
    "                annotation = []\n",
    "\n",
    "                for obj in root.findall(\"object\"):\n",
    "                    name = obj.find(\"name\").text\n",
    "                    bbox = obj.find(\"bndbox\")\n",
    "                    xmin = int(bbox.find(\"xmin\").text)\n",
    "                    ymin = int(bbox.find(\"ymin\").text)\n",
    "                    xmax = int(bbox.find(\"xmax\").text)\n",
    "                    ymax = int(bbox.find(\"ymax\").text)\n",
    "                    annotation.append((xmin, ymin, xmax, ymax))\n",
    "\n",
    "                self.annotations.append(annotation)\n",
    "            else:\n",
    "                self.annotations.append(None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "        annotation = self.annotations[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if annotation is not None:\n",
    "            boxes = torch.as_tensor(annotation, dtype=torch.float32)\n",
    "            labels = torch.ones((len(annotation),), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.empty((0, 4), dtype=torch.float32)\n",
    "            labels = torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "        return image, {\"boxes\": boxes, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bd8803b-ad51-490f-a357-b248ab6b1749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the dataset: 297\n",
      "Image shape: torch.Size([1, 800, 800])\n",
      "Target boxes: tensor([[367., 137., 407., 153.],\n",
      "        [238., 125., 279., 145.],\n",
      "        [ 95., 131., 135., 144.],\n",
      "        [578., 136., 607., 149.]])\n",
      "Target labels: tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    " # Test the dataset\n",
    "dataset_path = \"dataset\"  \n",
    "split = \"train\"  \n",
    "thermal_transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "thermal_dataset = CustomDataset(dataset_path, split, thermal_transform)\n",
    "print(f\"Number of images in the dataset: {len(thermal_dataset)}\")\n",
    "image, target = thermal_dataset[0]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Target boxes: {target['boxes']}\")\n",
    "print(f\"Target labels: {target['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a22f6551-03b4-49ce-aa75-28c340899847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed images: 297\n",
      "Number of preprocessed annotations: 297\n",
      "Preprocessed image shape: torch.Size([1, 800, 800])\n",
      "Preprocessed annotation boxes shape: torch.Size([4, 4])\n",
      "Preprocessed annotation labels shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Preprocessing\n",
    "def preprocess_dataset(dataset):\n",
    "    preprocessed_images = []\n",
    "    preprocessed_annotations = []\n",
    "    \n",
    "    for image, target in dataset:\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = transforms.ToPILImage()(image)\n",
    "        else:\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        image = thermal_transform(image)\n",
    "        \n",
    "        boxes = target['boxes']\n",
    "        labels = target['labels']\n",
    "        \n",
    "        _, height, width = image.shape\n",
    "        boxes[:, [0, 2]] /= width\n",
    "        boxes[:, [1, 3]] /= height\n",
    "        \n",
    "        target = {'boxes': boxes, 'labels': labels}\n",
    "        \n",
    "        preprocessed_images.append(image)\n",
    "        preprocessed_annotations.append(target)\n",
    "    \n",
    "    return preprocessed_images, preprocessed_annotations\n",
    "\n",
    "# Test the preprocessing function\n",
    "dataset_path = \"dataset\"\n",
    "split = \"train\"\n",
    "thermal_dataset = CustomDataset(dataset_path, split=split, transform=thermal_transform)\n",
    "preprocessed_images, preprocessed_annotations = preprocess_dataset(thermal_dataset)\n",
    "print(f\"Number of preprocessed images: {len(preprocessed_images)}\")\n",
    "print(f\"Number of preprocessed annotations: {len(preprocessed_annotations)}\")\n",
    "print(f\"Preprocessed image shape: {preprocessed_images[0].shape}\")\n",
    "print(f\"Preprocessed annotation boxes shape: {preprocessed_annotations[0]['boxes'].shape}\")\n",
    "print(f\"Preprocessed annotation labels shape: {preprocessed_annotations[0]['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bba4709a-4bfd-47cb-b4fb-13313163cb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: {'vehicle': 0}\n",
      "Batch images shape: torch.Size([4, 1, 800, 800])\n",
      "Batch targets boxes shape: torch.Size([5, 4])\n",
      "Batch targets labels shape: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataset and DataLoader Creation\n",
    "dataset_path = \"dataset\"\n",
    "split = \"train\"\n",
    "\n",
    "thermal_transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "thermal_dataset = CustomDataset(dataset_path, split, thermal_transform)\n",
    "\n",
    "class_labels = set()\n",
    "for _, annotation in thermal_dataset:\n",
    "    for obj in annotation:\n",
    "        name = obj[0]\n",
    "        class_labels.add(\"vehicle\")  \n",
    "\n",
    "class_to_idx = {\"vehicle\": 0}  \n",
    "print(\"Class labels:\", class_to_idx)\n",
    "\n",
    "preprocessed_thermal_images, preprocessed_thermal_annotations = preprocess_dataset(thermal_dataset)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    \n",
    "    images = torch.stack(images, dim=0)\n",
    "    \n",
    "    return images, targets\n",
    "\n",
    "train_thermal_dataset = list(zip(preprocessed_thermal_images, preprocessed_thermal_annotations))\n",
    "train_thermal_loader = DataLoader(train_thermal_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for images, targets in train_thermal_loader:\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Batch targets boxes shape: {targets[0]['boxes'].shape}\")\n",
    "    print(f\"Batch targets labels shape: {targets[0]['labels'].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3c6623a-9064-4031-ac93-d852c0ddb78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Thermal Loss: 0.4995\n",
      "Epoch [2/10], Thermal Loss: 0.2011\n",
      "Epoch [3/10], Thermal Loss: 0.2203\n",
      "Epoch [4/10], Thermal Loss: 0.1574\n",
      "Epoch [5/10], Thermal Loss: 0.1466\n",
      "Epoch [6/10], Thermal Loss: 0.1545\n",
      "Epoch [7/10], Thermal Loss: 0.1414\n",
      "Epoch [8/10], Thermal Loss: 0.1244\n",
      "Epoch [9/10], Thermal Loss: 0.1127\n",
      "Epoch [10/10], Thermal Loss: 0.1178\n",
      "Image 1 - Boxes: [[2.23930493e-01 1.95141882e-04 2.90383101e-01 9.22815949e-02]\n",
      " [2.80799270e-01 3.05030495e-04 3.56732011e-01 1.17054895e-01]\n",
      " [0.00000000e+00 6.94990158e-04 4.52226067e+00 2.56468511e+00]\n",
      " [2.46234685e-01 3.11119258e-02 3.22726220e-01 1.48866028e-01]\n",
      " [3.40033919e-01 2.61478126e-04 4.13563579e-01 8.37208480e-02]\n",
      " [4.06007826e-01 2.06591561e-04 4.86791015e-01 5.63654751e-02]\n",
      " [0.00000000e+00 2.32333586e-01 2.32761223e-02 3.60121608e-01]\n",
      " [4.94812243e-02 2.49403372e-01 1.18702486e-01 3.55359197e-01]\n",
      " [2.34523833e-01 3.13743949e-01 3.10134649e-01 4.40954447e-01]\n",
      " [2.78325021e-01 3.69665205e-01 3.50681007e-01 5.07565975e-01]\n",
      " [3.91270280e-01 3.22820991e-01 4.62816238e-01 4.61564869e-01]\n",
      " [6.19751990e-01 2.30613068e-01 6.77267015e-01 3.45112681e-01]\n",
      " [9.65054274e-01 3.04231048e-03 3.17211652e+00 6.79170609e-01]\n",
      " [7.96854794e-01 3.50066274e-01 8.66052806e-01 4.79142934e-01]\n",
      " [1.23655796e-02 6.03970885e-03 7.33888435e+00 5.69194317e-01]\n",
      " [1.13505518e+00 2.19113618e-01 1.22853553e+00 3.35906774e-01]\n",
      " [1.08724558e+00 3.03844154e-01 1.15000474e+00 4.30953145e-01]\n",
      " [9.26609576e-01 4.64144319e-01 1.01923454e+00 5.99134564e-01]\n",
      " [7.84605801e-01 6.76525354e-01 8.49690497e-01 8.05759788e-01]], Labels: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], Scores: [0.7731602  0.76969635 0.7660263  0.76599836 0.7653889  0.7588763\n",
      " 0.7251594  0.714361   0.652593   0.5969825  0.5964674  0.56392753\n",
      " 0.47878724 0.30196434 0.25601444 0.1726194  0.14766441 0.13284434\n",
      " 0.08534438]\n",
      "Image 2 - Boxes: [[0.30764967 0.24658076 0.38739842 0.3255286 ]\n",
      " [0.61862826 0.15178004 0.70141184 0.24388543]\n",
      " [0.35183555 0.31695497 0.43222988 0.39519143]\n",
      " [0.21042545 0.5035456  0.28978726 0.58345455]\n",
      " [0.63914424 0.30178416 0.71814686 0.3816114 ]\n",
      " [0.45167747 0.44186735 0.5310617  0.52062434]\n",
      " [0.36987576 0.54362243 0.46752772 0.6408872 ]\n",
      " [0.9885778  0.1672056  1.0737429  0.25225154]\n",
      " [0.7201141  0.46020123 0.79913175 0.5393249 ]\n",
      " [0.3037435  0.75124943 0.39302197 0.83956885]\n",
      " [0.37767774 0.7352203  0.45629275 0.8138163 ]\n",
      " [0.5423247  0.6633443  0.62296265 0.7519297 ]\n",
      " [0.91476876 0.4387402  0.9904861  0.5172433 ]\n",
      " [0.53323716 0.79008484 0.64045924 0.88121986]\n",
      " [0.9217996  0.5437797  1.0157996  0.645204  ]\n",
      " [0.6183744  0.83403563 0.63117164 0.85212135]], Labels: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], Scores: [0.6761666  0.62607306 0.60992366 0.49527305 0.4646251  0.44498107\n",
      " 0.38452512 0.29301178 0.23847581 0.22287142 0.20359625 0.18134856\n",
      " 0.14257173 0.10974334 0.088914   0.06920338]\n",
      "Image 3 - Boxes: [[4.76953983e-01 9.03718174e-05 5.31084538e-01 5.20992205e-02]\n",
      " [3.25856328e-01 1.82839423e-01 3.83559287e-01 2.44375318e-01]\n",
      " [3.82630885e-01 1.70014039e-01 4.37839568e-01 2.21679285e-01]\n",
      " [5.08170843e-01 1.14676192e-01 5.65364480e-01 1.72609493e-01]\n",
      " [6.29802644e-01 5.61955348e-02 6.87734783e-01 1.20185368e-01]\n",
      " [3.72145176e-01 2.27732986e-01 4.29162681e-01 2.81798571e-01]\n",
      " [7.36101151e-01 1.85090862e-02 7.96177387e-01 7.88629502e-02]\n",
      " [7.08340824e-01 9.00414959e-02 7.63253748e-01 1.48485303e-01]\n",
      " [6.25224292e-01 1.93814188e-01 6.79990232e-01 2.49654204e-01]\n",
      " [5.10953665e-01 2.77466446e-01 5.64830899e-01 3.36178988e-01]\n",
      " [4.01195973e-01 3.31017256e-01 4.55481023e-01 3.90035570e-01]\n",
      " [6.37172878e-01 2.11060777e-01 6.90318525e-01 2.68301785e-01]\n",
      " [4.27917063e-01 3.97829473e-01 4.95554447e-01 4.64788675e-01]\n",
      " [6.62396252e-01 3.34851652e-01 7.15843856e-01 3.91035944e-01]\n",
      " [9.66824055e-01 2.73044199e-01 1.03318095e+00 3.46967310e-01]\n",
      " [1.14346004e+00 1.02101445e-01 1.19264460e+00 1.66130960e-01]\n",
      " [7.65110612e-01 4.72640485e-01 8.27362299e-01 5.38576961e-01]\n",
      " [9.56331551e-01 3.40170532e-01 1.00913084e+00 4.04855758e-01]\n",
      " [3.51433754e-02 0.00000000e+00 5.46541643e+00 5.88969612e+00]\n",
      " [5.74604869e-01 7.33648717e-01 6.33202672e-01 8.00100386e-01]], Labels: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], Scores: [0.765242   0.72836125 0.7253661  0.72522026 0.7148828  0.6950121\n",
      " 0.675977   0.65103257 0.62520844 0.60678345 0.60439336 0.6019244\n",
      " 0.5272771  0.44829708 0.25174034 0.2501502  0.21877994 0.20242801\n",
      " 0.13218722 0.12575313]\n",
      "Image 4 - Boxes: [[1.63316727e-03 0.00000000e+00 4.39800119e+00 3.42175007e+00]\n",
      " [3.24170589e-01 3.20188701e-05 3.91984761e-01 4.45586070e-02]\n",
      " [2.37672061e-01 4.56824973e-02 2.97096640e-01 1.05936937e-01]\n",
      " [6.35591805e-01 1.23629346e-04 6.97340786e-01 3.37666944e-02]\n",
      " [3.26182783e-01 1.83787033e-01 3.73144388e-01 2.46052459e-01]\n",
      " [4.52269018e-01 2.24483341e-01 5.14335990e-01 2.82232672e-01]\n",
      " [6.79906487e-01 9.25416276e-02 7.38748312e-01 1.47923037e-01]\n",
      " [4.10041988e-01 3.09680820e-01 4.70882773e-01 3.71478260e-01]\n",
      " [2.07834587e-01 4.22493786e-01 2.71037698e-01 4.87482339e-01]\n",
      " [5.92565358e-01 2.78779745e-01 6.59012973e-01 3.49829733e-01]\n",
      " [5.23328960e-01 3.57561588e-01 5.80944002e-01 4.26767170e-01]\n",
      " [7.17453897e-01 3.14179242e-01 7.78865039e-01 3.79079640e-01]\n",
      " [8.15069318e-01 2.81825602e-01 8.80074024e-01 3.48421752e-01]\n",
      " [1.12091504e-01 6.91857696e-01 1.67222321e-01 7.50324607e-01]\n",
      " [8.55256021e-01 3.40426743e-01 9.13350642e-01 4.10310686e-01]\n",
      " [2.74384052e-01 7.39716768e-01 3.43322486e-01 8.17695975e-01]\n",
      " [1.15576267e+00 9.65164155e-02 1.20940232e+00 1.64607570e-01]\n",
      " [5.04455924e-01 6.74003661e-01 5.68933368e-01 7.40892351e-01]\n",
      " [3.53492975e-01 7.81108141e-01 4.16732430e-01 8.51034880e-01]\n",
      " [9.01655793e-01 4.36318994e-01 9.56344485e-01 5.06842613e-01]\n",
      " [1.75733805e-01 9.40941989e-01 2.35010058e-01 1.00403881e+00]\n",
      " [4.61983830e-01 8.49417686e-01 5.34952760e-01 9.26122189e-01]\n",
      " [4.66139317e-02 2.59296894e-02 7.02648354e+00 4.91571712e+00]\n",
      " [1.09809256e+00 3.86570394e-01 1.15997100e+00 4.59651649e-01]\n",
      " [4.96566743e-01 8.74196410e-01 5.61217487e-01 9.41393375e-01]\n",
      " [2.97801375e-01 1.07824504e+00 3.60011160e-01 1.13710153e+00]\n",
      " [4.43369150e-04 1.19959033e+00 5.65217845e-02 4.74660492e+00]], Labels: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], Scores: [0.79183537 0.79095364 0.7869279  0.7559858  0.75283873 0.71887225\n",
      " 0.7061695  0.6769525  0.64825505 0.628691   0.5918221  0.50214267\n",
      " 0.44897428 0.4397453  0.35081857 0.32357457 0.30021954 0.2653043\n",
      " 0.24756323 0.22202642 0.1945911  0.14801547 0.14595348 0.13915251\n",
      " 0.12137753 0.08405282 0.08220394]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Model Definition and Training\n",
    "num_classes = len(class_to_idx) + 1\n",
    "\n",
    "thermal_model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = thermal_model.roi_heads.box_predictor.cls_score.in_features\n",
    "thermal_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "thermal_model.to(device)\n",
    "\n",
    "thermal_optimizer = torch.optim.SGD(thermal_model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    thermal_model.train()\n",
    "    \n",
    "    thermal_epoch_loss = 0.0\n",
    "    \n",
    "    for thermal_images, thermal_targets in train_thermal_loader:\n",
    "        thermal_images = list(image.to(device) for image in thermal_images)\n",
    "        thermal_targets = [{k: v.to(device) for k, v in t.items()} for t in thermal_targets]\n",
    "        \n",
    "        thermal_loss_dict = thermal_model(thermal_images, thermal_targets)\n",
    "        thermal_losses = sum(loss for loss in thermal_loss_dict.values())\n",
    "        \n",
    "        thermal_optimizer.zero_grad()\n",
    "        thermal_losses.backward()\n",
    "        thermal_optimizer.step()\n",
    "        \n",
    "        thermal_epoch_loss += thermal_losses.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Thermal Loss: {thermal_epoch_loss/len(train_thermal_loader):.4f}\")\n",
    "\n",
    "torch.save(thermal_model.state_dict(), \"thermal_trained_model.pth\")\n",
    "\n",
    "thermal_model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in train_thermal_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        outputs = thermal_model(images)\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            boxes = outputs[i]['boxes'].cpu().numpy()\n",
    "            labels = outputs[i]['labels'].cpu().numpy()\n",
    "            scores = outputs[i]['scores'].cpu().numpy()\n",
    "            \n",
    "            print(f\"Image {i+1} - Boxes: {boxes}, Labels: {labels}, Scores: {scores}\")\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59b54a6a-d4d1-4a78-ae65-202639ddfcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test predictions: 43\n",
      "Number of test targets: 43\n",
      "Test prediction boxes shape: (12, 4)\n",
      "Test prediction labels shape: (12,)\n",
      "Test prediction scores shape: (12,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Evaluation and Testing\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for output in outputs:\n",
    "                boxes = output['boxes'].cpu().numpy()\n",
    "                labels = output['labels'].cpu().numpy()\n",
    "                scores = output['scores'].cpu().numpy()\n",
    "                \n",
    "                indices = torchvision.ops.nms(torch.tensor(boxes), torch.tensor(scores), iou_threshold=0.5)\n",
    "                \n",
    "                filtered_boxes = boxes[indices]\n",
    "                filtered_labels = labels[indices]\n",
    "                filtered_scores = scores[indices]\n",
    "                \n",
    "                all_predictions.append((filtered_boxes, filtered_labels, filtered_scores))\n",
    "            \n",
    "            for target in targets:\n",
    "                boxes = target['boxes'].cpu().numpy()\n",
    "                labels = target['labels'].cpu().numpy()\n",
    "                \n",
    "                all_targets.append((boxes, labels))\n",
    "    \n",
    "    return all_predictions, all_targets\n",
    "\n",
    "test_split = \"test\"\n",
    "test_thermal_dataset = CustomDataset(dataset_path, test_split, thermal_transform)\n",
    "test_thermal_loader = DataLoader(test_thermal_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_predictions, test_targets = evaluate_model(thermal_model, test_thermal_loader, device)\n",
    "print(f\"Number of test predictions: {len(test_predictions)}\")\n",
    "print(f\"Number of test targets: {len(test_targets)}\")\n",
    "print(f\"Test prediction boxes shape: {test_predictions[0][0].shape}\")\n",
    "print(f\"Test prediction labels shape: {test_predictions[0][1].shape}\")\n",
    "print(f\"Test prediction scores shape: {test_predictions[0][2].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3ba4cbe-2489-43b6-8927-980eabef9771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 7: Load the trained model\n",
    "thermal_model.load_state_dict(torch.load(\"thermal_trained_model.pth\"))\n",
    "thermal_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc26e762-2ad1-4d30-aabe-95c025616211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Prepare the test dataset\n",
    "test_split = \"test\"\n",
    "test_thermal_dataset = CustomDataset(dataset_path, test_split, thermal_transform)\n",
    "test_thermal_loader = DataLoader(test_thermal_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9280ef03-8e8f-4743-b14b-477da8ea1db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [1]\n",
      "Boxes: [[1.63316727e-03 0.00000000e+00 4.39800119e+00 3.42175007e+00]\n",
      " [3.24170589e-01 3.20188701e-05 3.91984761e-01 4.45586070e-02]\n",
      " [2.37672061e-01 4.56824973e-02 2.97096640e-01 1.05936937e-01]\n",
      " [6.35591805e-01 1.23629346e-04 6.97340786e-01 3.37666944e-02]\n",
      " [3.26182783e-01 1.83787033e-01 3.73144388e-01 2.46052459e-01]\n",
      " [4.52269018e-01 2.24483341e-01 5.14335990e-01 2.82232672e-01]\n",
      " [6.79906487e-01 9.25416276e-02 7.38748312e-01 1.47923037e-01]\n",
      " [4.10041988e-01 3.09680820e-01 4.70882773e-01 3.71478260e-01]\n",
      " [2.07834587e-01 4.22493786e-01 2.71037698e-01 4.87482339e-01]\n",
      " [5.92565358e-01 2.78779745e-01 6.59012973e-01 3.49829733e-01]\n",
      " [5.23328960e-01 3.57561588e-01 5.80944002e-01 4.26767170e-01]\n",
      " [7.17453897e-01 3.14179242e-01 7.78865039e-01 3.79079640e-01]\n",
      " [8.15069318e-01 2.81825602e-01 8.80074024e-01 3.48421752e-01]\n",
      " [1.12091504e-01 6.91857696e-01 1.67222321e-01 7.50324607e-01]\n",
      " [8.55256021e-01 3.40426743e-01 9.13350642e-01 4.10310686e-01]\n",
      " [2.74384052e-01 7.39716768e-01 3.43322486e-01 8.17695975e-01]\n",
      " [1.15576267e+00 9.65164155e-02 1.20940232e+00 1.64607570e-01]\n",
      " [5.04455924e-01 6.74003661e-01 5.68933368e-01 7.40892351e-01]\n",
      " [3.53492975e-01 7.81108141e-01 4.16732430e-01 8.51034880e-01]\n",
      " [9.01655793e-01 4.36318994e-01 9.56344485e-01 5.06842613e-01]\n",
      " [1.75733805e-01 9.40941989e-01 2.35010058e-01 1.00403881e+00]\n",
      " [4.61983830e-01 8.49417686e-01 5.34952760e-01 9.26122189e-01]\n",
      " [4.66139317e-02 2.59296894e-02 7.02648354e+00 4.91571712e+00]\n",
      " [1.09809256e+00 3.86570394e-01 1.15997100e+00 4.59651649e-01]\n",
      " [4.96566743e-01 8.74196410e-01 5.61217487e-01 9.41393375e-01]\n",
      " [2.97801375e-01 1.07824504e+00 3.60011160e-01 1.13710153e+00]\n",
      " [4.43369150e-04 1.19959033e+00 5.65217845e-02 4.74660492e+00]]\n",
      "Labels: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Scores: [0.79183537 0.79095364 0.7869279  0.7559858  0.75283873 0.71887225\n",
      " 0.7061695  0.6769525  0.64825505 0.628691   0.5918221  0.50214267\n",
      " 0.44897428 0.4397453  0.35081857 0.32357457 0.30021954 0.2653043\n",
      " 0.24756323 0.22202642 0.1945911  0.14801547 0.14595348 0.13915251\n",
      " 0.12137753 0.08405282 0.08220394]\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Evaluate the model on the test dataset\n",
    "test_predictions, test_targets = evaluate_model(thermal_model, test_thermal_loader, device)\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "print(f\"Boxes: {boxes}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "254214f9-4be4-4092-85c0-5e9b058d3c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxes: [[0.34770954 0.04174948 0.40519255 0.12731211]\n",
      " [0.42938927 0.01595853 0.4804578  0.09012715]\n",
      " [0.15285218 0.24474478 0.2219496  0.33774346]\n",
      " [0.7196178  0.07688493 0.7793055  0.15435433]\n",
      " [0.4142287  0.37830088 0.48401204 0.45835814]\n",
      " [0.22703454 0.5933771  0.2888647  0.67999935]\n",
      " [0.79546416 0.25732532 0.8478794  0.34908983]\n",
      " [0.3645666  0.5934224  0.41686067 0.68178606]\n",
      " [0.8320304  0.29618055 0.9021169  0.38711792]\n",
      " [1.1690063  0.20589468 1.2163565  0.29787752]\n",
      " [0.8595847  0.5023668  0.91113377 0.5867882 ]\n",
      " [0.9134542  0.5000308  0.96142614 0.5903397 ]\n",
      " [1.1557858  0.36425036 1.216984   0.45716965]]\n",
      "Labels: [1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Scores: [0.76316464 0.7598545  0.712865   0.62550384 0.5295133  0.39359325\n",
      " 0.3854628  0.32807457 0.3132534  0.14409949 0.12858258 0.10668069\n",
      " 0.07779296]\n",
      "Boxes: [[1.8165039e-01 2.4084933e-05 2.1950047e-01 1.9544084e-02]\n",
      " [3.3406541e-06 5.8238138e-02 1.2824725e-02 1.0889727e-01]\n",
      " [2.1256378e-01 4.8016261e-02 2.5328329e-01 9.2824623e-02]\n",
      " [3.8297901e-01 2.9089395e-05 4.1226473e-01 1.1006536e-02]\n",
      " [1.5179875e-01 1.0042090e-01 1.8987390e-01 1.5101653e-01]\n",
      " [3.1849176e-01 7.3229529e-02 3.5293311e-01 1.2544985e-01]\n",
      " [3.6953223e-01 1.3666418e-01 4.1084856e-01 1.8598658e-01]\n",
      " [6.0222852e-01 1.5651369e-01 6.3976860e-01 2.0899159e-01]\n",
      " [8.2271343e-01 7.4214548e-02 8.5697335e-01 1.2938635e-01]\n",
      " [0.0000000e+00 8.6197853e-03 5.2968025e+00 4.6854405e+00]\n",
      " [9.9491221e-01 1.0324836e-02 2.7577832e+00 2.2349305e+00]\n",
      " [7.5856888e-01 2.4708797e-01 7.9750514e-01 2.9450980e-01]\n",
      " [3.6451325e-01 5.6585455e-01 4.0690598e-01 6.1031926e-01]\n",
      " [1.1570629e+00 2.7854089e-03 1.1900831e+00 4.2667814e-02]]\n",
      "Labels: [1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Scores: [0.7971993  0.7907522  0.78752106 0.78735524 0.77980304 0.7771118\n",
      " 0.75583434 0.68772537 0.6038104  0.5791376  0.50585717 0.498252\n",
      " 0.40076128 0.33713105]\n",
      "Boxes: [[0.32018855 0.02277737 0.37818894 0.10907917]\n",
      " [0.43681234 0.02283769 0.48808843 0.09740286]\n",
      " [0.16394472 0.24131636 0.2316429  0.3323465 ]\n",
      " [0.725263   0.07173981 0.78542423 0.14983866]\n",
      " [0.4727327  0.40252745 0.54112303 0.48051393]\n",
      " [0.7764685  0.2528166  0.8288268  0.34423462]\n",
      " [0.7439038  0.28979695 0.81264573 0.3783689 ]\n",
      " [0.22943659 0.5993354  0.29034626 0.6844482 ]\n",
      " [0.32905695 0.5762447  0.3812308  0.66463065]\n",
      " [1.1423484  0.20349354 1.1901618  0.29617083]\n",
      " [0.8233633  0.48850584 0.874418   0.5718725 ]\n",
      " [0.88313174 0.4761998  0.9308336  0.5657949 ]\n",
      " [1.1000807  0.35230395 1.1604605  0.44375268]]\n",
      "Labels: [1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Scores: [0.7702752  0.7570214  0.7124683  0.6251899  0.47272262 0.40723863\n",
      " 0.39620462 0.38568556 0.36218464 0.16147813 0.15523271 0.13270542\n",
      " 0.10303888]\n",
      "Boxes: [[0.31844574 0.02677232 0.37628555 0.11256976]\n",
      " [0.44541883 0.02637735 0.49659938 0.1005744 ]\n",
      " [0.14611307 0.2432886  0.2136496  0.33391747]\n",
      " [0.72957045 0.07831878 0.789596   0.15606266]\n",
      " [0.48039055 0.41242665 0.5486671  0.48998952]\n",
      " [0.3408115  0.52883106 0.39272344 0.6158578 ]\n",
      " [0.7804953  0.2567074  0.8328173  0.34775302]\n",
      " [0.73678994 0.29491717 0.8053677  0.3830458 ]\n",
      " [0.22365503 0.6037003  0.2847439  0.6887903 ]\n",
      " [1.1379249  0.20552978 1.1856318  0.29765633]\n",
      " [0.8211504  0.49385175 0.8722739  0.5771146 ]\n",
      " [0.89196616 0.48014897 0.9397474  0.5694192 ]\n",
      " [1.0940113  0.35779828 1.1542864  0.4488725 ]]\n",
      "Labels: [1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Scores: [0.76945597 0.75465363 0.7137358  0.61782885 0.4575455  0.40528888\n",
      " 0.39977795 0.39712638 0.38360766 0.16314945 0.15299863 0.12646048\n",
      " 0.10308027]\n",
      "Boxes: [[2.13816240e-01 2.91145630e-02 2.54312664e-01 7.34007955e-02]\n",
      " [8.93762857e-02 8.81661475e-02 1.27320588e-01 1.38309047e-01]\n",
      " [3.15723211e-01 6.34284988e-02 3.49183589e-01 1.14704825e-01]\n",
      " [2.85883069e-01 1.40815064e-01 3.25525522e-01 1.89239964e-01]\n",
      " [0.00000000e+00 0.00000000e+00 4.36068535e+00 4.48127508e+00]\n",
      " [5.36760569e-01 1.48562089e-01 5.73055744e-01 2.00044587e-01]\n",
      " [7.96080410e-01 6.37079999e-02 8.30058396e-01 1.18315242e-01]\n",
      " [9.22762215e-01 1.38539076e-02 2.70753026e+00 2.94758940e+00]\n",
      " [7.65952706e-01 2.38137141e-01 8.04632902e-01 2.85089374e-01]\n",
      " [1.82475954e-01 5.86231053e-01 2.23592401e-01 6.29815996e-01]\n",
      " [1.22555995e+00 1.68915093e-03 2.73633265e+00 4.93879616e-01]]\n",
      "Labels: [1 1 1 1 1 1 1 1 1 1 1]\n",
      "Scores: [0.7911673  0.783963   0.7796212  0.7634689  0.7485314  0.7142841\n",
      " 0.6317822  0.57149154 0.50150025 0.4625215  0.28324467]\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Visualize the object detection results\n",
    "def visualize_detections(image, boxes, labels, scores, class_labels, confidence_threshold=0.3):\n",
    "    image_with_detections = image.copy()\n",
    "    \n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    if not isinstance(boxes, (list, np.ndarray)):\n",
    "        boxes = [boxes]\n",
    "    \n",
    "    if not isinstance(labels, (list, np.ndarray)):\n",
    "        labels = [labels]\n",
    "    \n",
    "    if not isinstance(scores, (list, np.ndarray)):\n",
    "        scores = [scores]\n",
    "    \n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= confidence_threshold:\n",
    "            if isinstance(box, (list, np.ndarray)):\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "            else:\n",
    "                xmin, ymin, xmax, ymax = box, box, box, box  \n",
    "            xmin = int(xmin * width)\n",
    "            ymin = int(ymin * height)\n",
    "            xmax = int(xmax * width)\n",
    "            ymax = int(ymax * height)\n",
    "            \n",
    "            class_name = class_labels[int(label)]  \n",
    "            \n",
    "            cv2.rectangle(image_with_detections, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            cv2.putText(image_with_detections, f\"{class_name}: {score:.2f}\", (xmin, ymin - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return image_with_detections\n",
    "\n",
    "num_visualizations = 5\n",
    "class_labels = {1: \"vehicle\"}\n",
    "\n",
    "for i in range(num_visualizations):\n",
    "    image_path = test_thermal_dataset.images[i]\n",
    "    image = cv2.imread(image_path)  \n",
    "    \n",
    "    boxes, labels, scores = test_predictions[i]\n",
    "    \n",
    "    print(f\"Boxes: {boxes}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(f\"Scores: {scores}\")\n",
    "    \n",
    "    image_with_detections = visualize_detections(image, boxes, labels, scores, class_labels)\n",
    "    \n",
    "    cv2.imshow(f\"Thermal Object Detection - Image {i+1}\", image_with_detections)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b4064a-6a69-4374-b29c-82ebe4ced3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 224, 224])\n",
      "Label: -1\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Image Classification Dataset\n",
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if \"vehicle\" in image_path:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = -1  \n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "vehicle_image_paths = [os.path.join(\"dataset\", \"train\", \"images\", img) for img in os.listdir(os.path.join(\"dataset\", \"train\", \"images\"))]\n",
    "classification_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset = ImageClassificationDataset(vehicle_image_paths, transform=classification_transform)\n",
    "image, label = dataset[0]\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bac8e0ae-3e3d-4cf4-9a22-e1e993274132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([16, 3, 224, 224])\n",
      "Labels: tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Image Classification Dataset Creation\n",
    "classification_dataset = ImageClassificationDataset(vehicle_image_paths, transform=classification_transform)\n",
    "classification_dataloader = DataLoader(classification_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for images, labels in classification_dataloader:\n",
    "    print(\"Batch shape:\", images.shape)\n",
    "    print(\"Labels:\", labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6f331e4-4228-408a-8af2-0a83226528f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Image Classification Model\n",
    "class_names = [\"vehicle\"]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "classification_model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "num_features = classification_model.fc.in_features\n",
    "classification_model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "classification_model.to(device)\n",
    "\n",
    "# Capture the model architecture summary in a string\n",
    "model_summary = str(classification_model)\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d874c5ef-934e-49a3-a255-376b9c77a3ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m classification_model(images)\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_criterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m classification_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Image Classification Training\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "classification_optimizer = torch.optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classification_model.train()\n",
    "    \n",
    "    for images, labels in classification_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = classification_model(images)\n",
    "        loss = classification_criterion(outputs, labels)\n",
    "        \n",
    "        classification_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        classification_optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Classification Loss: {loss.item():.4f}\")\n",
    "\n",
    "torch.save(classification_model.state_dict(), \"classification_model.pth\")\n",
    "\n",
    "classification_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in classification_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = classification_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Classification Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914f91d-2a86-4821-8ed4-08e4927dfc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4dd190-3e05-4ed6-8570-f075992cdde3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
